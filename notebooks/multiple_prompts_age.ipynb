{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "attribute = \"education\"\n",
    "probe_layer = 26\n",
    "layers = list(range(0, probe_layer+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/theo-env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading from thorsley/user_modelling_probes_gemma-9b-it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/user_modelling/utils/load_probes.py:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  weights = torch.load(weights_file, map_location=device)\n",
      "/root/user_modelling/utils/load_probes.py:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  biases = torch.load(bias_file, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "from utils.load_probes import load_probe\n",
    "from utils.probes import make_probes_for_each_layer\n",
    "import torch\n",
    "probes = load_probe(\n",
    "    attribute\n",
    ")\n",
    "weights, biases = probes\n",
    "# weights = torch.load(\"./collected_gender_probe_weights.pt\", map_location=\"cuda\")\n",
    "# biases = torch.load(\"./collected_gender_probe_biases.pt\", map_location=\"cuda\")\n",
    "probes_for_each_layer = make_probes_for_each_layer(weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, text in enumerate(texts):\n",
    "#     texts[i] = text + \" **\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constantSetting center_unembed=False instead.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  4.38it/s]\n",
      "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model google/gemma-2-9b-it into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "import transformer_lens as tl\n",
    "import torch\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "model_name = f\"google/gemma-2-9b-it\"\n",
    "model = tl.HookedTransformer.from_pretrained(model_name, center_unembed=True, dtype=\"bfloat16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.probes import load_dataset_for_model\n",
    "\n",
    "texts, labels = load_dataset_for_model(model, attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.probes import LinearProbes\n",
    "from utils.index import Ix\n",
    "\n",
    "def probe_attribution_metric(\n",
    "    cache: tl.ActivationCache | dict,  \n",
    "    probe: LinearProbes,  \n",
    "    hook_point: str, \n",
    "    correct_label: int | list[int],\n",
    "    pos_slice: slice | None = Ix[:, -1].as_index,\n",
    "):\n",
    "    if pos_slice is None:\n",
    "        resid_cache = cache[hook_point]\n",
    "    else:\n",
    "        resid_cache = cache[hook_point][pos_slice]\n",
    "    probe = probe.to(resid_cache.device).to(dtype=resid_cache.dtype)\n",
    "    probe_logits = probe.probe(resid_cache)\n",
    "    if len(probe_logits.shape) == 2:\n",
    "        if isinstance(correct_label, list):\n",
    "            assert len(correct_label) == probe_logits.shape[0]\n",
    "            correct_logits = probe_logits[torch.arange(probe_logits.shape[0]), correct_label]\n",
    "\n",
    "            all_labels = list(range(probe.probe.bias.shape[0]))\n",
    "            wrong_labels = []\n",
    "            for label in correct_label:\n",
    "                wrong_labels.append(all_labels.remove(label))\n",
    "            wrong_labels = torch.tensor(wrong_labels).mean(dim=-1)\n",
    "            #wrong_logits = probe_logits[torch.arange(probe_logits.shape[0]), wrong_labels]\n",
    "            raise ValueError(\"AGHHHH\")\n",
    "            return correct_logits - wrong_labels\n",
    "\n",
    "\n",
    "        wrong_label = list(range(probe.probe.bias.shape[0]))\n",
    "        wrong_label.remove(correct_label)\n",
    "        return probe_logits[:, correct_label] - probe_logits[:, wrong_label].mean(dim=-1)\n",
    "    else:\n",
    "        wrong_label = list(range(probe.probe.bias.shape[0]))\n",
    "        wrong_label.remove(correct_label)\n",
    "        return probe_logits[correct_label] - probe_logits[wrong_label].mean(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from utils.cache import get_cache_fwd_and_bwd\n",
    "\n",
    "probe = probes_for_each_layer[probe_layer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAE Attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.sae_loader import load_gemma_saes\n",
    "saes = load_gemma_saes(\"9b\", layers=layers[:-1])\n",
    "saes.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = list(range(0, probe_layer+1))\n",
    "hook_points = [f\"blocks.{l_no}.hook_resid_post\" for l_no in layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.attribution import compute_sae_activations_and_attributions\n",
    "from tqdm import tqdm\n",
    "\n",
    "all_sae_attrs = []\n",
    "all_sae_acts = []\n",
    "for prompt_idx in tqdm(range(len(texts[:50]))):\n",
    "    prompt = texts[prompt_idx]\n",
    "    correct_label = labels[prompt_idx]\n",
    "    metric = partial(probe_attribution_metric, probe=probe, hook_point=f\"blocks.{probe_layer}.hook_resid_post\", correct_label=correct_label)\n",
    "    loss, fwd_cache, bwd_cache = get_cache_fwd_and_bwd(model, prompt, metric, hook_points=hook_points, metric_needs_cache=True)\n",
    "\n",
    "    sae_acts, sae_attrs = compute_sae_activations_and_attributions(\n",
    "        saes, fwd_cache, bwd_cache, hook_points[:-1]\n",
    "    )\n",
    "    \n",
    "    all_sae_attrs.append(sae_attrs)\n",
    "    all_sae_acts.append(sae_acts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.attribution import get_top_k_contributions\n",
    "import numpy as np\n",
    "\n",
    "top_k_dfs = []\n",
    "for attr in all_sae_attrs:\n",
    "    # attr: [layers, positions, latents]\n",
    "    \n",
    "    # Select positions with the highest attribution score\n",
    "    per_pos_contribution = attr[:, 1:, :].sum(-1) # layer x positions\n",
    "    top_k_contributions = get_top_k_contributions(per_pos_contribution, k=5)[\"latent_idx\"].tolist()\n",
    "    tokens_we_care_about, occurrences = np.unique(top_k_contributions, return_counts=True)\n",
    "    \n",
    "    # Select latents with the highest attribution score at those positions\n",
    "    per_latent_contribution = attr[:, tokens_we_care_about].sum(1) # layer x latents\n",
    "    df = get_top_k_contributions(per_latent_contribution)\n",
    "    top_k_dfs.append(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_dfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# with open(\"notebooks/results/top_k_dfs.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(top_k_dfs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_combined = pd.concat(top_k_dfs)\n",
    "\n",
    "mean_contribs = df_combined.groupby([\"layer\", \"latent_idx\"]).mean()\n",
    "latents = mean_contribs.index.to_numpy()\n",
    "contribs = mean_contribs[\"contribution\"]\n",
    "\n",
    "top_k_attrs = np.zeros((max(df_combined[\"layer\"]) + 1, max(df_combined[\"latent_idx\"]) + 100))\n",
    "\n",
    "for idx, contrib in zip(latents, contribs):\n",
    "    layer, latent_idx = idx\n",
    "    top_k_attrs[layer, latent_idx] = contrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.neel_plotly import line\n",
    "line(top_k_attrs, title=\"Mean contribution of each latent to the probe accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line((abs(top_k_attrs) > 0).sum(axis=-1), title=\"Number of latents used by layer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line((abs(top_k_attrs)).sum(axis=-1) / (abs(top_k_attrs) > 0).sum(axis=-1), title=\"Mean contribution of each latent to the probe accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at where these latents activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nodewise Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.neel_utils as nutils\n",
    "from utils.attribution import attr_patch_head_vector\n",
    "from utils.probe_attribution import make_corrupt_cache_with_correct_shape\n",
    "from IPython.display import Markdown\n",
    "from utils.neel_plotly import line, imshow\n",
    "import einops\n",
    "from utils.cache import make_mean_cache\n",
    "from utils.probe_attribution import compute_model_attribution_patching_scores_with_mean_cache as get_AP_scores\n",
    "from utils.attribution import compute_attention_attribution_for_prompt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "LAYERS = list(range(0, probe_layer+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:27<00:00,  1.84it/s]\n"
     ]
    }
   ],
   "source": [
    "mean_cache = make_mean_cache(model, texts[:100], batch_size=2, padding_side=\"right\", device=\"cuda\", hook_point_substring=[\"resid\", \"attn\", \"mlp\"])\n",
    "mean_cache.keys()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_substr = [\"man\", \"guy\", \"men\", \"boys\", \"my wife\"]\n",
    "female_substr = [\"woman\", \"girl\", \"women\", \"girls\", \"my husband\", \"feminine\", \"female\"]\n",
    "\n",
    "def has_gendered_substr(text):\n",
    "    for substr in male_substr:\n",
    "        for word in text.lower().split(' '):\n",
    "            if substr == word.strip(\".,!?\"):\n",
    "                return True\n",
    "    for substr in female_substr:\n",
    "        for word in text.lower().split(' '):\n",
    "            if substr == word.strip(\".,!?\"):\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Prompt Nodewise Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_idx = 8\n",
    "logits = model.forward(texts[prompt_idx])\n",
    "logits = logits[:, -1]\n",
    "nutils.show_df(nutils.create_vocab_df(logits[0], make_probs=True).head(15))\n",
    "print(texts[prompt_idx])\n",
    "print(\"Label:\", labels[prompt_idx])\n",
    "#print(\"has gendered substr:\", has_gendered_substr(texts[prompt_idx]))\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resample ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "from utils.activation_patching import make_ablation_hook\n",
    "from tqdm import tqdm\n",
    "\n",
    "def patch_resid_at_each_token_pos(\n",
    "    model: tl.HookedTransformer,\n",
    "    clean_prompt: str,\n",
    "    corrupted_cache: dict,\n",
    "    metric: Callable,\n",
    "    use_resid_post: bool = False,\n",
    "    probe_layer: int = 26,\n",
    "    layer_interval: int = 1\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Run node-wise patching across all layers and positions.\n",
    "\n",
    "    Args:\n",
    "        model: The transformer model\n",
    "        clean_prompt: The clean input prompt\n",
    "        counterfactual_prompt: The corrupted input prompt\n",
    "        corrupted_cache: Cache from running the corrupted input\n",
    "        resid_hook_points: List of residual hook points to patch\n",
    "        metric: Metric function to evaluate outputs\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape (n_layers, seq_len) containing patching results\n",
    "    \"\"\"\n",
    "    seq_len = len(model.to_str_tokens(clean_prompt))\n",
    "    n_layers = probe_layer+1\n",
    "    results = torch.zeros(n_layers, seq_len)\n",
    "    resid_hook_points = [\n",
    "        (\n",
    "            f\"blocks.{i}.hook_resid_pre\"\n",
    "            if not use_resid_post\n",
    "            else f\"blocks.{i}.hook_resid_post\"\n",
    "        )\n",
    "        for i in range(n_layers)\n",
    "    ]\n",
    "    caching_hook_point = f\"blocks.{probe_layer}.hook_resid_post\"\n",
    "\n",
    "    def make_cache_hook(cache: dict):\n",
    "        def hook_fn(act, hook):\n",
    "            cache[hook.name] = act.detach().clone().to(\"cpu\")\n",
    "        return hook_fn\n",
    "\n",
    "    for layer in range(0, n_layers, layer_interval):\n",
    "        print(f\"Layer {layer}\")\n",
    "        for pos in tqdm(range(seq_len)):\n",
    "            torch.cuda.empty_cache()\n",
    "            hook_point = resid_hook_points[layer]\n",
    "            cache = {}\n",
    "            out = model.run_with_hooks(\n",
    "                clean_prompt,\n",
    "                fwd_hooks=[\n",
    "                    (\n",
    "                        hook_point,\n",
    "                        make_ablation_hook(cache=corrupted_cache, index=Ix[:, pos]),\n",
    "                    ),\n",
    "                    (caching_hook_point, make_cache_hook(cache))\n",
    "                ],\n",
    "            )\n",
    "            results[layer, pos] = metric(cache).item()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "results = patch_resid_at_each_token_pos(\n",
    "    model=model,\n",
    "    clean_prompt=texts[prompt_idx],\n",
    "    corrupted_cache=mean_cache,\n",
    "    metric=metric,\n",
    "    layer_interval=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.neel_utils import imshow\n",
    "\n",
    "LABELED_TOKENS = nutils.process_tokens_index(model.to_str_tokens(texts[prompt_idx]))\n",
    "fig = imshow(results[::2], x=LABELED_TOKENS, y=[f\"Layer {i}\" for i in range(0, results.shape[0], 2)], return_fig=True)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### resid attribution patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = []\n",
    "for i, text in enumerate(texts[:10]):\n",
    "    prompts.append(text + \"**\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_resid_attrs = []\n",
    "hook_points = [f\"blocks.{layer}.hook_resid_pre\" for layer in LAYERS] + [f\"blocks.{probe_layer}.hook_resid_post\"]\n",
    "all_losses = []\n",
    "for prompt_idx in tqdm(range(len(texts[:10]))):\n",
    "    prompt = texts[prompt_idx]\n",
    "    correct_label = labels[prompt_idx]\n",
    "    metric = partial(probe_attribution_metric, probe=probe, hook_point=f\"blocks.{probe_layer}.hook_resid_post\", correct_label=correct_label)\n",
    "    loss, fwd_cache, bwd_cache = get_cache_fwd_and_bwd(model, prompt, metric, hook_points=hook_points, metric_needs_cache=True)\n",
    "    all_losses.append(loss)\n",
    "    resid_attrs = get_AP_scores(\n",
    "        model, fwd_cache, bwd_cache, mean_cache, hook_points[:-1]\n",
    "    )\n",
    "    \n",
    "    all_resid_attrs.append(resid_attrs)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line(all_losses, title=\"Loss for each prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_idx = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(all_resid_attrs[prompt_idx][:,:-1].sum(-1), title=\"Sum of Attribution Patching Scores for each token\", x=nutils.process_tokens_index(model.to_str_tokens(texts[prompt_idx]))[:-1], y=hook_points[:-1], width=1500, height=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resid, MLP, and Attn Patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = texts[prompt_idx]\n",
    "LABELED_TOKENS = nutils.process_tokens_index(model.to_str_tokens(prompt))\n",
    "correct_label = labels[prompt_idx]\n",
    "metric = partial(\n",
    "    probe_attribution_metric,\n",
    "    probe=probe,\n",
    "    hook_point=f\"blocks.{probe_layer}.hook_resid_post\",\n",
    "    correct_label=correct_label,\n",
    ")\n",
    "resid_hook_points = [f\"blocks.{layer}.hook_resid_pre\" for layer in LAYERS]\n",
    "mlp_hook_points = [f\"blocks.{layer}.hook_mlp_out\" for layer in LAYERS]\n",
    "attn_hook_points = [f\"blocks.{layer}.hook_attn_out\" for layer in LAYERS]\n",
    "\n",
    "_, fwd_cache, bwd_cache = get_cache_fwd_and_bwd(\n",
    "    model,\n",
    "    prompt,\n",
    "    metric,\n",
    "    hook_points=resid_hook_points\n",
    "    + mlp_hook_points\n",
    "    + attn_hook_points\n",
    "    + [f\"blocks.{probe_layer}.hook_resid_post\"],\n",
    "    metric_needs_cache=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_attributions = get_AP_scores(\n",
    "    model=model,\n",
    "    clean_fwd_cache=fwd_cache,\n",
    "    clean_bwd_cache=bwd_cache,\n",
    "    mean_cache=mean_cache,\n",
    "    hook_points=resid_hook_points,\n",
    ")\n",
    "\n",
    "mlp_attributions = get_AP_scores(\n",
    "    model=model,\n",
    "    clean_fwd_cache=fwd_cache,\n",
    "    clean_bwd_cache=bwd_cache,\n",
    "    mean_cache=mean_cache,\n",
    "    hook_points=mlp_hook_points,\n",
    ")\n",
    "\n",
    "attn_attributions = get_AP_scores(\n",
    "    model=model,\n",
    "    clean_fwd_cache=fwd_cache,\n",
    "    clean_bwd_cache=bwd_cache,\n",
    "    mean_cache=mean_cache,\n",
    "    hook_points=attn_hook_points,\n",
    ")\n",
    "\n",
    "imshow(\n",
    "    resid_attributions.sum(-1)[:,:-1],\n",
    "    x=LABELED_TOKENS[:-1],\n",
    "    y=resid_hook_points,\n",
    "    title=\"Per layer Resid Attribution Patching Scores\",\n",
    ")\n",
    "imshow(\n",
    "    mlp_attributions.sum(-1)[:,:-1],\n",
    "    x=LABELED_TOKENS[:-1],\n",
    "    y=mlp_hook_points,\n",
    "    title=\"Per layer MLP Attribution Patching Scores\",\n",
    ")\n",
    "imshow(\n",
    "    attn_attributions.sum(-1)[:,:-1],\n",
    "    x=LABELED_TOKENS[:-1],\n",
    "    y=attn_hook_points,\n",
    "    title=\"Per layer Attn Attribution Patching Scores\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attn Head Patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_attn_hook_points = [f\"blocks.{layer}.attn.hook_q\" for layer in LAYERS]\n",
    "all_attn_hook_points += [f\"blocks.{layer}.attn.hook_k\" for layer in LAYERS]\n",
    "all_attn_hook_points += [f\"blocks.{layer}.attn.hook_v\" for layer in LAYERS]\n",
    "all_attn_hook_points += [f\"blocks.{layer}.attn.hook_z\" for layer in LAYERS]\n",
    "\n",
    "caching_hook_points = all_attn_hook_points + [f\"blocks.{probe_layer}.hook_resid_post\"]\n",
    "prompt = texts[prompt_idx]\n",
    "correct_label = labels[prompt_idx]\n",
    "LABELED_TOKENS = nutils.process_tokens_index(model.to_str_tokens(prompt))\n",
    "metric = partial(\n",
    "    probe_attribution_metric,\n",
    "    probe=probe,\n",
    "    hook_point=f\"blocks.{probe_layer}.hook_resid_post\",\n",
    "    correct_label=correct_label,\n",
    ")\n",
    "\n",
    "_, clean_cache, bwd_cache = get_cache_fwd_and_bwd(model, prompt, metric=metric, hook_points=caching_hook_points, metric_needs_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELED_TOKENS = nutils.process_tokens_index(model.to_str_tokens(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.attribution import attr_patch_head_vector\n",
    "from utils.plots import plot_head_vector_attribution\n",
    "from IPython.display import Markdown\n",
    "from utils.neel_plotly import line\n",
    "import einops\n",
    "head_vector_attr_dict = {}\n",
    "use_corrupt_bwd_cache = False\n",
    "corrupt_cache = make_corrupt_cache_with_correct_shape(mean_cache, caching_hook_points, clean_cache, model)\n",
    "for activation_name, activation_name_full in [\n",
    "    (\"k\", \"Key\"),\n",
    "    (\"q\", \"Query\"),\n",
    "    (\"v\", \"Value\"),\n",
    "    (\"z\", \"Mixed Value\"),\n",
    "]:\n",
    "    heads = list(range(clean_cache[activation_name, 0].shape[2]))\n",
    "    layers = list(range(probe_layer))\n",
    "    head_vector_attr_dict[activation_name], head_vector_labels = attr_patch_head_vector(\n",
    "        model, clean_cache, corrupt_cache, bwd_cache, activation_name, layers=layers, heads=heads, using_corrupt_grad_cache=use_corrupt_bwd_cache\n",
    "    )\n",
    "\n",
    "    plot_head_vector_attribution(head_vector_attr_dict[activation_name], head_vector_labels, activation_name_full, layers, heads, LABELED_TOKENS, sort=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attn Attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_patterns, attn_attrs  = compute_attention_attribution_for_prompt(model, prompt, metric, return_acts=True, metric_needs_cache=True, cache_hook_point=f\"blocks.{probe_layer}.hook_resid_post\", layers=LAYERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "LABELED_TOKENS = nutils.process_tokens_index(model.to_str_tokens(prompt))\n",
    "imshow(attn_attrs.sum([0, 1]), x=LABELED_TOKENS, y=LABELED_TOKENS, title=\"Attribution Mean Scores\")\n",
    "imshow(attn_attrs.std([0, 1]), x=LABELED_TOKENS, y=LABELED_TOKENS, title=\"Attribution Stds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 19\n",
    "head = 3\n",
    "imshow(attn_patterns[layer, head], x=LABELED_TOKENS, y=LABELED_TOKENS, title=f\"Layer {layer} Head {head} Attention Patterns\")\n",
    "imshow(attn_attrs[layer, head], x=LABELED_TOKENS, y=LABELED_TOKENS, title=f\"Layer {layer} Head {head} Attribution Scores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Prompt Top Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_prompts = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:47<00:00,  2.14s/it]\n"
     ]
    }
   ],
   "source": [
    "all_mlp_AP_scores = []\n",
    "all_attn_AP_scores = []\n",
    "\n",
    "all_attn_hook_points = [f\"blocks.{layer}.attn.hook_q\" for layer in LAYERS]\n",
    "all_attn_hook_points += [f\"blocks.{layer}.attn.hook_k\" for layer in LAYERS]\n",
    "all_attn_hook_points += [f\"blocks.{layer}.attn.hook_v\" for layer in LAYERS]\n",
    "all_attn_hook_points += [f\"blocks.{layer}.attn.hook_z\" for layer in LAYERS]\n",
    "\n",
    "mlp_hook_points = [f\"blocks.{layer}.hook_mlp_out\" for layer in LAYERS]\n",
    "skipped_prompts = []\n",
    "for prompt_idx in tqdm(range(num_prompts)):\n",
    "    prompt = texts[prompt_idx]\n",
    "    correct_label = labels[prompt_idx]\n",
    "    metric = partial(\n",
    "        probe_attribution_metric,\n",
    "        probe=probe,\n",
    "        hook_point=f\"blocks.{probe_layer}.hook_resid_post\",\n",
    "        correct_label=correct_label,\n",
    "    )\n",
    "\n",
    "    loss, fwd_cache, bwd_cache = get_cache_fwd_and_bwd(\n",
    "        model,\n",
    "        prompt,\n",
    "        metric,\n",
    "        hook_points= mlp_hook_points + [f\"blocks.{probe_layer}.hook_resid_post\"],\n",
    "        metric_needs_cache=True,\n",
    "    )\n",
    "\n",
    "\n",
    "    if loss < 0.5:\n",
    "        print(f\"Prompt {prompt_idx} has loss {loss}, skipping\")\n",
    "        skipped_prompts.append(prompt_idx)\n",
    "        continue\n",
    "\n",
    "    mlp_attributions = get_AP_scores(\n",
    "        model=model,\n",
    "        clean_fwd_cache=fwd_cache,\n",
    "        clean_bwd_cache=bwd_cache,\n",
    "        mean_cache=mean_cache,\n",
    "        hook_points=mlp_hook_points,\n",
    "    )\n",
    "    all_mlp_AP_scores.append(mlp_attributions)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    metric = partial(\n",
    "        probe_attribution_metric,\n",
    "        probe=probe,\n",
    "        hook_point=f\"blocks.{probe_layer}.hook_resid_post\",\n",
    "        correct_label=correct_label,\n",
    "    )\n",
    "\n",
    "    caching_hook_points = all_attn_hook_points + [f\"blocks.{probe_layer}.hook_resid_post\"]\n",
    "\n",
    "    _, clean_cache, bwd_cache = get_cache_fwd_and_bwd(model, prompt, metric=metric, hook_points=caching_hook_points, metric_needs_cache=True)\n",
    "\n",
    "    head_vector_attr_dict = {}\n",
    "    for activation_name in [\"z\", \"v\", \"k\", \"q\"]:\n",
    "        heads = list(range(clean_cache[activation_name, 0].shape[2]))\n",
    "        corrupt_cache = make_corrupt_cache_with_correct_shape(mean_cache, caching_hook_points, clean_cache, model)\n",
    "        head_vector_attr_dict[activation_name], head_vector_labels = attr_patch_head_vector(\n",
    "            model, clean_cache, corrupt_cache, bwd_cache, activation_name, using_corrupt_grad_cache=False,\n",
    "            layers=LAYERS, heads=heads\n",
    "        )\n",
    "    all_attn_AP_scores.append(head_vector_attr_dict)\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_attn_AP_scores), len(all_mlp_AP_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_attn_AP_scores[0]['z'].shape, all_attn_AP_scores[0]['q'].shape, all_attn_AP_scores[0]['k'].shape, all_attn_AP_scores[0]['v'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/thorsley/AP_dump/commit/3ae8a3385799578593efe1ea6b0c3a3362ee7e2e', commit_message='Upload all_mlp_AP_scores_education.pkl with huggingface_hub', commit_description='', oid='3ae8a3385799578593efe1ea6b0c3a3362ee7e2e', pr_url=None, repo_url=RepoUrl('https://huggingface.co/thorsley/AP_dump', endpoint='https://huggingface.co', repo_type='model', repo_id='thorsley/AP_dump'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "import pickle\n",
    "\n",
    "# Convert to bytes\n",
    "import io\n",
    "buffer = io.BytesIO()\n",
    "pickle.dump(all_mlp_AP_scores, buffer)\n",
    "buffer.seek(0)\n",
    "\n",
    "# Upload to HF\n",
    "api = HfApi()\n",
    "api.upload_file(\n",
    "    path_or_fileobj=buffer,\n",
    "    path_in_repo=f\"all_mlp_AP_scores_{attribute}.pkl\",\n",
    "    repo_id=\"thorsley/AP_dump\",\n",
    "    repo_type=\"model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.index import Ix\n",
    "\n",
    "def get_sum_head_vector_attrs(all_attn_AP_scores, clean_cache, layers: list[int], pos_slice: slice | None = None):\n",
    "    all_sum_head_vector_attrs = {}\n",
    "    for attn_score in all_attn_AP_scores:\n",
    "        for key, value in attn_score.items():\n",
    "            if key not in all_sum_head_vector_attrs:\n",
    "                all_sum_head_vector_attrs[key] = []\n",
    "            sum_head_vector_attr = einops.reduce(\n",
    "                value if pos_slice is None else value[pos_slice],\n",
    "                \"(layer head) pos -> layer head\",\n",
    "                \"sum\",\n",
    "                layer=len(layers),\n",
    "                head=clean_cache[key, 0].shape[2],\n",
    "            )\n",
    "            all_sum_head_vector_attrs[key].append(sum_head_vector_attr)\n",
    "\n",
    "    for key, value in all_sum_head_vector_attrs.items():\n",
    "        all_sum_head_vector_attrs[key] = torch.stack(value, dim=0)\n",
    "    return all_sum_head_vector_attrs\n",
    "\n",
    "all_sum_head_vector_attrs = get_sum_head_vector_attrs(all_attn_AP_scores, clean_cache, LAYERS)\n",
    "sum_head_vectors_first_half = get_sum_head_vector_attrs(all_attn_AP_scores, clean_cache, LAYERS, pos_slice=Ix[:, :-9].as_index)\n",
    "sum_head_vectors_second_half = get_sum_head_vector_attrs(all_attn_AP_scores, clean_cache, LAYERS, pos_slice=Ix[:, -10:].as_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(all_sum_head_vector_attrs[\"k\"].mean(dim=0), xaxis=\"Latent Index\", yaxis=\"Layer\", title=\"Mean Attribution of Top Key Heads\")\n",
    "imshow(all_sum_head_vector_attrs[\"k\"].std(dim=0), xaxis=\"Latent Index\", yaxis=\"Layer\", title=\"Std Attribution of Top Key Heads\")\n",
    "# imshow(all_sum_head_vector_attrs[\"k\"].min(dim=0).values, xaxis=\"Latent Index\", yaxis=\"Layer\", title=\"Min Abs Attribution of Top Key Heads\")\n",
    "# imshow(all_sum_head_vector_attrs[\"k\"].max(dim=0).values, xaxis=\"Latent Index\", yaxis=\"Layer\", title=\"Max Abs Attribution of Top Key Heads\")\n",
    "imshow(all_sum_head_vector_attrs[\"q\"].mean(dim=0), xaxis=\"Latent Index\", yaxis=\"Layer\", title=\"Mean Attribution of Top Query Heads\")\n",
    "imshow(all_sum_head_vector_attrs[\"q\"].std(dim=0), xaxis=\"Latent Index\", yaxis=\"Layer\", title=\"Std Attribution of Top Query Heads\")\n",
    "# imshow(all_sum_head_vector_attrs[\"q\"].min(dim=0).values, xaxis=\"Latent Index\", yaxis=\"Layer\", title=\"Min Abs Attribution of Top Query Heads\")\n",
    "# imshow(all_sum_head_vector_attrs[\"q\"].max(dim=0).values, xaxis=\"Latent Index\", yaxis=\"Layer\", title=\"Max Abs Attribution of Top Query Heads\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(sum_head_vectors_first_half[\"k\"].mean(dim=0), xaxis=\"Latent Index\", yaxis=\"Layer\", title=\"Mean Attribution of Key Heads (First Half)\")\n",
    "# imshow(sum_head_vectors_first_half[\"k\"].std(dim=0), xaxis=\"Latent Index\", yaxis=\"Layer\", title=\"Std Attribution of Key Heads (First Half)\")\n",
    "imshow(sum_head_vectors_second_half[\"k\"].mean(dim=0), xaxis=\"Latent Index\", yaxis=\"Layer\", title=\"Mean Attribution of Key Heads (Second Half)\")\n",
    "# imshow(sum_head_vectors_first_half[\"q\"].std(dim=0), xaxis=\"Latent Index\", yaxis=\"Layer\", title=\"Std Attribution of Query Heads (First Half)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(sum_head_vectors_first_half[\"q\"].mean(dim=0), xaxis=\"Latent Index\", yaxis=\"Layer\", title=\"Mean Attribution of Query Heads (first Half)\")\n",
    "# imshow(sum_head_vectors_second_half[\"k\"].std(dim=0), xaxis=\"Latent Index\", yaxis=\"Layer\", title=\"Std Attribution of Key Heads (second Half)\")\n",
    "imshow(sum_head_vectors_second_half[\"q\"].mean(dim=0), xaxis=\"Latent Index\", yaxis=\"Layer\", title=\"Mean Attribution of Query Heads (second Half)\")\n",
    "# imshow(sum_head_vectors_second_half[\"q\"].std(dim=0), xaxis=\"Latent Index\", yaxis=\"Layer\", title=\"Std Attribution of Query Heads (First Half)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_attn_AP_scores[0][\"z\"].shape, len(all_attn_AP_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(texts[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "def get_contributions(all_attn_AP_scores, clean_cache, layer, head):\n",
    "    contribution_list = {}\n",
    "    for attn_score in all_attn_AP_scores:\n",
    "        for key, value in attn_score.items():\n",
    "            head_to_use = head * clean_cache[key, 0].shape[2] // model.cfg.n_heads\n",
    "            deconcatenated = einops.rearrange(\n",
    "                value,\n",
    "                \"(layer head) pos -> layer head pos\",\n",
    "                layer=len(LAYERS),\n",
    "                head=clean_cache[key, 0].shape[2],\n",
    "            )\n",
    "            if key not in contribution_list:\n",
    "                contribution_list[key] = []\n",
    "            contribution_list[key].append(deconcatenated[layer, head_to_use])\n",
    "\n",
    "    for key, list_of_contributions in contribution_list.items():\n",
    "        fig = go.Figure()\n",
    "        i = 0\n",
    "        for contribution in list_of_contributions:\n",
    "            while i in skipped_prompts:\n",
    "                i += 1\n",
    "            if i > 8:\n",
    "                break\n",
    "\n",
    "            labeled_tokens = nutils.process_tokens_index(model.to_str_tokens(texts[i]))\n",
    "            # Add hover text showing the token at each position\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=torch.linspace(0, 1, len(contribution)),\n",
    "                    y=contribution.to(float),\n",
    "                    mode=\"lines\",  # Added markers to make hovering easier\n",
    "                    name=f\"Prompt {i}\",\n",
    "                    text=labeled_tokens,  # Add tokens as hover text\n",
    "                    hovertemplate=\"Position: %{x:.2f}<br>\" +\n",
    "                                \"Attribution: %{y:.4f}<br>\" +\n",
    "                                \"Token: %{text}<br>\" +\n",
    "                                f\"Has gendered substr: {has_gendered_substr(texts[i])}<br>\" +\n",
    "                                f\"Label: {labels[i]}<br>\" +\n",
    "                                \"<extra></extra>\"  # Removes trace name from hover\n",
    "                )\n",
    "            )\n",
    "            i += 1\n",
    "        fig.update_layout(\n",
    "            title=f\"{key} Attribution of Head {head} for Layer {layer}\",\n",
    "            xaxis_title=\"Normalised Position\",\n",
    "            yaxis_title=\"Attribution\",\n",
    "        )\n",
    "        fig.show()\n",
    "\n",
    "\n",
    "head = 2\n",
    "layer = 26\n",
    "get_contributions(all_attn_AP_scores, clean_cache, layer, head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "head = 4\n",
    "layer = 26\n",
    "# head = 5\n",
    "# layer = 18\n",
    "# head = 7\n",
    "# layer = 3\n",
    "# get_contributions(all_attn_AP_scores, clean_cache, layer, head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.attribution import compute_attention_attribution_for_prompt\n",
    "all_clean_attn_patterns = []\n",
    "all_clean_attn_attrs = []\n",
    "for prompt_idx in tqdm(range(num_prompts)):\n",
    "    if prompt_idx in skipped_prompts:\n",
    "        continue\n",
    "    prompt = texts[prompt_idx]\n",
    "    correct_label = labels[prompt_idx]\n",
    "    attn_patterns, attn_attrs = compute_attention_attribution_for_prompt(model, prompt, metric, return_acts=True, metric_needs_cache=True, cache_hook_point=f\"blocks.{probe_layer}.hook_resid_post\", layers=LAYERS + [27])\n",
    "    all_clean_attn_patterns.append(attn_patterns)\n",
    "    all_clean_attn_attrs.append(attn_attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bwd_cache.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# head = 1\n",
    "# layer = 25\n",
    "# i = 0\n",
    "# for (pattern, attr) in zip(all_clean_attn_patterns, all_clean_attn_attrs):\n",
    "#     labeled_tokens = nutils.process_tokens_index(model.to_str_tokens(texts[i]))\n",
    "#     i += 1\n",
    "#     if i > 5:\n",
    "#         break\n",
    "#     while i in skipped_prompts:\n",
    "#         i += 1\n",
    "#     imshow(pattern[layer, head], x=labeled_tokens, y=labeled_tokens, title=f\"Layer {layer} Head {head} Attention Pattern\")\n",
    "#     imshow(attr[layer, head], x=labeled_tokens, y=labeled_tokens, title=f\"Layer {layer} Head {head} Attribution Scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.attribution import get_top_k_contributions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "threshold = len(all_attn_AP_scores) / 4\n",
    "top_k_attn_attrs = []\n",
    "for attn_score in all_attn_AP_scores:\n",
    "    att_score_z = attn_score[\"z\"]\n",
    "    sum_head_vector_attr = einops.reduce(\n",
    "        att_score_z,\n",
    "        \"(layer head) pos -> layer head\",\n",
    "        \"sum\",\n",
    "        layer=len(LAYERS),\n",
    "        head=model.cfg.n_heads,\n",
    "    )\n",
    "\n",
    "    top_k_attn_attrs.append(get_top_k_contributions(-sum_head_vector_attr, k=4, absolute=False))\n",
    "\n",
    "top_k_combined = pd.concat(top_k_attn_attrs)\n",
    "mean_contribs = top_k_combined.groupby([\"layer\", \"latent_idx\"]).mean()\n",
    "\n",
    "idxs = mean_contribs.index.to_numpy()\n",
    "contribs = mean_contribs[\"contribution\"]\n",
    "\n",
    "top_k_attrs = np.zeros((max(top_k_combined[\"layer\"]) + 1, 16))\n",
    "\n",
    "for idx, contrib in zip(idxs, contribs):\n",
    "    layer, latent_idx = idx\n",
    "    top_k_attrs[layer, latent_idx] = contrib\n",
    "\n",
    "occurences_df = top_k_combined.groupby([\"layer\", \"latent_idx\"]).count()\n",
    "idxs = occurences_df.index.to_numpy()\n",
    "occurences = occurences_df[\"contribution\"]\n",
    "top_k_counts = np.zeros((max(top_k_combined[\"layer\"]) + 1, 16))\n",
    "for idx, count in zip(idxs, occurences):\n",
    "    layer, latent_idx = idx\n",
    "    top_k_counts[layer, latent_idx] = count\n",
    "\n",
    "top_k_counts_thresholded = top_k_counts * (top_k_counts >= threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imshow(top_k_attrs, title=\"Mean contribution for top k heads\")\n",
    "# imshow(top_k_counts, title=\"Occurences of top k heads\")\n",
    "# imshow(top_k_counts_thresholded, title=\"Occurences of top k heads (thresholded)\")\n",
    "# imshow(top_k_attrs * (top_k_counts_thresholded > 0), title=\"Mean contribution for top k heads (thresholded)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"notebooks/results/top_k_attn_attrs.pkl\", \"wb\") as f:\n",
    "    pickle.dump(top_k_attn_attrs, f)\n",
    "\n",
    "with open(\"notebooks/results/all_attn_AP_scores.pkl\", \"wb\") as f:\n",
    "    pickle.dump(all_attn_AP_scores, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
