{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "attribute = \"gender\"\n",
    "probe_layer = 28\n",
    "layers = list(range(0, probe_layer+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/user_modelling/.venv/lib/python3.11/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "537bef84dfe848248038e58b618648d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "collected_gender_probe_weights.pt:   0%|          | 0.00/1.21M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e640a8134c64114b87b6a99ad63f203",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "collected_gender_probe_biases.pt:   0%|          | 0.00/1.71k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/user_modelling/utils/load_probes.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  weights = torch.load(weights_file, map_location=device)\n",
      "/workspace/user_modelling/utils/load_probes.py:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  biases = torch.load(bias_file, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "from utils.load_probes import load_probe\n",
    "from utils.probes import make_probes_for_each_layer\n",
    "\n",
    "probes = load_probe(\n",
    "    attribute\n",
    ")\n",
    "\n",
    "weights, biases = probes\n",
    "probes_for_each_layer = make_probes_for_each_layer(weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.probes import load_dataset\n",
    "\n",
    "texts, labels = load_dataset(attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constantSetting center_unembed=False instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bbec3d474ed44ff8224f10172ebfe4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89996175aff74f27b12849b323b90339",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model google/gemma-2-9b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "import transformer_lens as tl\n",
    "import torch\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "model_name = f\"google/gemma-2-9b\"\n",
    "model = tl.HookedTransformer.from_pretrained(model_name, center_unembed=True, dtype=\"bfloat16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.probes import LinearProbes\n",
    "from utils.index import Ix\n",
    "\n",
    "def probe_attribution_metric(\n",
    "    cache: tl.ActivationCache | dict,  \n",
    "    probe: LinearProbes,  \n",
    "    hook_point: str, \n",
    "    correct_label: int | list[int],\n",
    "    pos_slice: slice | None = Ix[:, -1].as_index\n",
    "):\n",
    "    if pos_slice is None:\n",
    "        resid_cache = cache[hook_point]\n",
    "    else:\n",
    "        resid_cache = cache[hook_point][pos_slice]\n",
    "    probe = probe.to(resid_cache.device).to(dtype=resid_cache.dtype)\n",
    "    probe_logits = probe.probe(resid_cache)\n",
    "    if len(probe_logits.shape) == 2:\n",
    "        if isinstance(correct_label, list):\n",
    "            assert len(correct_label) == probe_logits.shape[0]\n",
    "            return probe_logits[torch.arange(probe_logits.shape[0]), correct_label]\n",
    "        return probe_logits[:, correct_label]\n",
    "    else:\n",
    "        return probe_logits[correct_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from utils.cache import get_cache_fwd_and_bwd\n",
    "\n",
    "probe = probes_for_each_layer[probe_layer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAE Attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc47e45aad2a4fd1a58293a5bd071faf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading SAEs:   0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.sae_loader import load_gemma_saes\n",
    "saes = load_gemma_saes(\"9b\", layers=layers[:-1])\n",
    "saes.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = list(range(0, probe_layer+1))\n",
    "hook_points = [f\"blocks.{l_no}.hook_resid_post\" for l_no in layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:17<00:00,  2.76s/it]\n"
     ]
    }
   ],
   "source": [
    "from utils.attribution import compute_sae_activations_and_attributions\n",
    "from tqdm import tqdm\n",
    "\n",
    "all_sae_attrs = []\n",
    "all_sae_acts = []\n",
    "for prompt_idx in tqdm(range(len(texts[:50]))):\n",
    "    prompt = texts[prompt_idx]\n",
    "    correct_label = labels[prompt_idx]\n",
    "    metric = partial(probe_attribution_metric, probe=probe, hook_point=f\"blocks.{probe_layer}.hook_resid_post\", correct_label=correct_label)\n",
    "    loss, fwd_cache, bwd_cache = get_cache_fwd_and_bwd(model, prompt, metric, hook_points=hook_points, metric_needs_cache=True)\n",
    "\n",
    "    sae_acts, sae_attrs = compute_sae_activations_and_attributions(\n",
    "        saes, fwd_cache, bwd_cache, hook_points[:-1]\n",
    "    )\n",
    "    \n",
    "    all_sae_attrs.append(sae_attrs)\n",
    "    all_sae_acts.append(sae_acts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, fwd_cache, bwd_cache = get_cache_fwd_and_bwd(model, prompt, metric, hook_points=hook_points, metric_needs_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['blocks.0.hook_resid_post', 'blocks.1.hook_resid_post', 'blocks.2.hook_resid_post', 'blocks.3.hook_resid_post', 'blocks.4.hook_resid_post', 'blocks.5.hook_resid_post', 'blocks.6.hook_resid_post', 'blocks.7.hook_resid_post', 'blocks.8.hook_resid_post', 'blocks.9.hook_resid_post', 'blocks.10.hook_resid_post', 'blocks.11.hook_resid_post', 'blocks.12.hook_resid_post', 'blocks.13.hook_resid_post', 'blocks.14.hook_resid_post', 'blocks.15.hook_resid_post', 'blocks.16.hook_resid_post', 'blocks.17.hook_resid_post', 'blocks.18.hook_resid_post', 'blocks.19.hook_resid_post', 'blocks.20.hook_resid_post', 'blocks.21.hook_resid_post', 'blocks.22.hook_resid_post', 'blocks.23.hook_resid_post', 'blocks.24.hook_resid_post', 'blocks.25.hook_resid_post', 'blocks.26.hook_resid_post', 'blocks.27.hook_resid_post', 'blocks.28.hook_resid_post']),\n",
       " '\\n',\n",
       " dict_keys(['blocks.27.hook_resid_post', 'blocks.26.hook_resid_post', 'blocks.25.hook_resid_post', 'blocks.24.hook_resid_post', 'blocks.23.hook_resid_post', 'blocks.22.hook_resid_post', 'blocks.21.hook_resid_post', 'blocks.20.hook_resid_post', 'blocks.19.hook_resid_post', 'blocks.18.hook_resid_post', 'blocks.17.hook_resid_post', 'blocks.16.hook_resid_post', 'blocks.15.hook_resid_post', 'blocks.14.hook_resid_post', 'blocks.13.hook_resid_post', 'blocks.12.hook_resid_post', 'blocks.11.hook_resid_post', 'blocks.10.hook_resid_post', 'blocks.9.hook_resid_post', 'blocks.8.hook_resid_post', 'blocks.7.hook_resid_post', 'blocks.6.hook_resid_post', 'blocks.5.hook_resid_post', 'blocks.4.hook_resid_post', 'blocks.3.hook_resid_post', 'blocks.2.hook_resid_post', 'blocks.1.hook_resid_post', 'blocks.0.hook_resid_post']))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fwd_cache.keys(), \"\\n\", bwd_cache.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.attribution import get_top_k_contributions\n",
    "import numpy as np\n",
    "\n",
    "top_k_dfs = []\n",
    "for attr in all_sae_attrs:\n",
    "    per_pos_contribution = attr[:, 1:, :].sum(-1) # layer x positions\n",
    "    top_k_contributions = get_top_k_contributions(per_pos_contribution, k=5)[\"latent_idx\"].tolist()\n",
    "    tokens_we_care_about, occurrences = np.unique(top_k_contributions, return_counts=True)\n",
    "    per_latent_contribution = attr[:, tokens_we_care_about].sum(1) # layer x latents\n",
    "    df = get_top_k_contributions(per_latent_contribution)\n",
    "    top_k_dfs.append(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latent_idx</th>\n",
       "      <th>layer</th>\n",
       "      <th>contribution</th>\n",
       "      <th>abs_contribution</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1891</td>\n",
       "      <td>0</td>\n",
       "      <td>1.249013</td>\n",
       "      <td>1.249013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1587</td>\n",
       "      <td>0</td>\n",
       "      <td>0.947196</td>\n",
       "      <td>0.947196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15624</td>\n",
       "      <td>0</td>\n",
       "      <td>0.786435</td>\n",
       "      <td>0.786435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15335</td>\n",
       "      <td>0</td>\n",
       "      <td>0.652231</td>\n",
       "      <td>0.652231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6497</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.529762</td>\n",
       "      <td>0.529762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>14008</td>\n",
       "      <td>27</td>\n",
       "      <td>0.740169</td>\n",
       "      <td>0.740169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>6600</td>\n",
       "      <td>27</td>\n",
       "      <td>-0.565594</td>\n",
       "      <td>0.565594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>13238</td>\n",
       "      <td>27</td>\n",
       "      <td>-0.423949</td>\n",
       "      <td>0.423949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>12486</td>\n",
       "      <td>27</td>\n",
       "      <td>-0.421051</td>\n",
       "      <td>0.421051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>1432</td>\n",
       "      <td>27</td>\n",
       "      <td>-0.399129</td>\n",
       "      <td>0.399129</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>140 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     latent_idx  layer  contribution  abs_contribution\n",
       "0          1891      0      1.249013          1.249013\n",
       "1          1587      0      0.947196          0.947196\n",
       "2         15624      0      0.786435          0.786435\n",
       "3         15335      0      0.652231          0.652231\n",
       "4          6497      0     -0.529762          0.529762\n",
       "..          ...    ...           ...               ...\n",
       "135       14008     27      0.740169          0.740169\n",
       "136        6600     27     -0.565594          0.565594\n",
       "137       13238     27     -0.423949          0.423949\n",
       "138       12486     27     -0.421051          0.421051\n",
       "139        1432     27     -0.399129          0.399129\n",
       "\n",
       "[140 rows x 4 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k_dfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"notebooks/results/top_k_dfs.pkl\", \"wb\") as f:\n",
    "    pickle.dump(top_k_dfs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nodewise Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
